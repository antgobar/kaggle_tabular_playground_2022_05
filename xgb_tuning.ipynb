{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helpers.helper_functions import load_data, encode_string_value, get_prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data('data')\n",
    "X,y = train.drop('target', axis=1), train.target\n",
    "X = get_prepared_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Objective(trial):\n",
    "\n",
    "    \n",
    "    # n_estimators (int) – Number of gradient boosted trees.\n",
    "    # max_depth (int) – Maximum tree depth for base learners.\n",
    "    # learning_rate (float) – Boosting learning rate.\n",
    "    # booster (string) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "    # tree_method (string) – Specify which tree method to use.\n",
    "    # gamma (float) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    # min_child_weight (float) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "    # max_delta_step (float) – Maximum delta step we allow each tree’s weight estimation to be.\n",
    "    # subsample (float) – Subsample ratio of the training instance.\n",
    "    # colsample_bytree (float) – Subsample ratio of columns when constructing each tree.\n",
    "    # colsample_bylevel (float) – Subsample ratio of columns for each level.\n",
    "    # colsample_bynode (float) – Subsample ratio of columns for each split.\n",
    "    # reg_alpha (float) – L1 regularization term on weights\n",
    "    # reg_lambda (float) – L2 regularization term on weights\n",
    "\n",
    "\n",
    "    param = {\n",
    "        \"tree_method\": \"gpu_hist\",  # use gpu\n",
    "        \"objective\":\"binary:hinge\", #Hinge loss may give better accuracy and some sparsity but much less sensitivity in terms of probabilities\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-3, 10.0),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-3, 10.0),\n",
    "        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.6),\n",
    "        \"subsample\":  trial.suggest_loguniform(\"subsample\", 0.4, 0.8),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.05),\n",
    "        \"n_estimators\": 8000,\n",
    "        'n_jobs' : -1,\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "        \"random_state\": 42,\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 100),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "    CV_score_array = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_valid = X[train_index], X[test_index]\n",
    "        y_train, y_valid = y[train_index], y[test_index]\n",
    "        clf = xgb.XGBClassifier(**param, use_label_encoder=False)\n",
    "        clf.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "        )\n",
    "\n",
    "        preds = clf.predict(X_valid)\n",
    "        auc = roc_auc_score(y_valid, preds)\n",
    "\n",
    "        CV_score_array.append(auc)\n",
    "    avg = np.mean(CV_score_array)\n",
    "    return avg"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bde5e1f7b796a682a06c8eaba79816a1d991d2e6fa8c73d37f152ca65472f55c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
